# Base Model Configuration
base_model:
  name: "meta-llama/Llama-3-8B-Instruct"  # Primary choice
  alternatives:
    - "ai4bharat/IndicLegal-LLaMA-7B"
    - "ai4bharat/IndicBERT"
  max_length: 4096
  device: "cuda"  # or "cpu"

# Tokenization
tokenization:
  strategy: "multilingual_legal"
  languages: ["hi", "en"]
  special_tokens:
    - "<IPC>"
    - "<CrPC>"
    - "<Constitution>"
    - "<Section>"
    - "<Article>"
    - "<Case>"
    - "<Citation>"
  max_vocab_size: 50000

# Legal Vocabulary
legal_vocab:
  ipc_sections: true
  crpc_sections: true
  constitution_articles: true
  case_law: true
  legal_terms_file: "data/legal_terms.txt"

# Embedding Model
embedding:
  model_name: "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
  dimension: 768
  batch_size: 32

# RAG Configuration
rag:
  vector_store: "faiss"  # or "elasticsearch"
  index_type: "flat"  # or "ivf", "hnsw"
  top_k: 5
  rerank: true
  rerank_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

# LoRA Configuration
lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# Training Configuration
training:
  batch_size: 4
  gradient_accumulation_steps: 8
  learning_rate: 2e-4
  num_epochs: 3
  warmup_steps: 100
  max_grad_norm: 1.0
  save_steps: 500
  eval_steps: 500
  logging_steps: 100

# RLHF Configuration
rlhf:
  ppo:
    learning_rate: 1e-5
    batch_size: 4
    mini_batch_size: 2
    ppo_epochs: 4
    cliprange: 0.2
    cliprange_value: 0.2
    gamma: 1.0
    lam: 0.95
  
  dpo:
    learning_rate: 1e-5
    beta: 0.1
    loss_type: "sigmoid"

# Reward Model
reward_model:
  legal_factuality_weight: 0.4
  citation_accuracy_weight: 0.3
  language_fluency_weight: 0.2
  safety_weight: 0.1

# Multi-Bot Architecture
multi_bot:
  legal_q_bot:
    model: "lora_legal_model"
    temperature: 0.7
    max_tokens: 512
  
  citation_bot:
    model: "citation_extractor"
    temperature: 0.3
    max_tokens: 256
  
  translation_bot:
    model: "indic-transliteration"
    temperature: 0.5
    max_tokens: 512
  
  validator_bot:
    model: "rlhf_validator"
    temperature: 0.1
    max_tokens: 128

